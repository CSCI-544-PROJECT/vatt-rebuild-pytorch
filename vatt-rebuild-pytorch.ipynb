{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Install necessary libraries\n",
    "!pip -q install torchaudio transformers PySoundFile tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The modified VATT architecture for audio-text alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install and Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Audio Tokenization\n",
    "class AudioTokenizer(nn.Module):\n",
    "    def __init__(self, patch_size=128, embed_dim=2048):\n",
    "        super(AudioTokenizer, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Linear(patch_size, embed_dim)\n",
    "    \n",
    "    def forward(self, audio_signal):\n",
    "        # Assuming audio_signal is [batch, time_samples]\n",
    "        batch_size, time_len = audio_signal.shape\n",
    "        num_patches = time_len // self.patch_size\n",
    "        audio_signal = audio_signal[:, :num_patches * self.patch_size]\n",
    "        audio_patches = audio_signal.reshape(batch_size, num_patches, self.patch_size)\n",
    "        # Project each patch to embedding dimension\n",
    "        audio_embeddings = self.projection(audio_patches)  # [batch, num_patches, embed_dim]\n",
    "        return audio_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(nn.Module):\n",
    "    def __init__(self, embed_dim=768, max_length=512):\n",
    "        super(TextTokenizer, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, embed_dim)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text is a list of strings\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens[\"input_ids\"].to(device)  # [batch, max_length]\n",
    "        text_embeddings = self.embedding(input_ids)  # [batch, max_length, embed_dim]\n",
    "        return text_embeddings  # Ensures 3D output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape [1, max_len, embed_dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adjust the positional encoding to match the actual input shape\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        pe = self.pe[:, :seq_len, :].expand(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return x + pe.to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Transformer Encoder Components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim * expansion_factor)\n",
    "        self.fc2 = nn.Linear(embed_dim * expansion_factor, embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.activation(self.fc1(x)))\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.linear1 = nn.Linear(embed_dim, embed_dim * 4)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear2 = nn.Linear(embed_dim * 4, embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFactory(nn.Module):\n",
    "    \"\"\"\n",
    "    Factory class to create TransformerEncoder instances with customizable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, feedforward_dim_multiplier=4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            num_layers (int): Number of Transformer layers.\n",
    "            feedforward_dim_multiplier (int): Multiplier for feedforward layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(TransformerFactory, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=embed_dim * feedforward_dim_multiplier, \n",
    "                dropout=dropout,\n",
    "                batch_first=True  # Enable batch-first optimization\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Projection Head\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, proj_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.hidden_dim = embed_dim * 2\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embed_dim, self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_dim, proj_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract [CLS] token (or average pool)\n",
    "        x = x[:, 0]  # Assuming first token is [CLS]\n",
    "        return self.projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Contrastive Learning (NCE and MIL-NCE)\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, features_a, features_b):\n",
    "        # Normalize features and compute similarity matrix\n",
    "        features_a = nn.functional.normalize(features_a, dim=1)\n",
    "        features_b = nn.functional.normalize(features_b, dim=1)\n",
    "        logits = torch.matmul(features_a, features_b.T) / self.temperature\n",
    "        labels = torch.arange(len(features_a)).to(device)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: DropToken Implementation\n",
    "class DropToken(nn.Module):\n",
    "    def __init__(self, drop_rate=0.5):\n",
    "        super(DropToken, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_rate\n",
    "        mask = torch.rand(x.shape[:2], device=x.device) < keep_prob\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assuming we have already downloaded TED-LIUM dataset and extracted audio + text files\n",
    "# From: https://www.openslr.org/51/ [50.6 GB]\n",
    "PATH_TO_AUDIO_FILES = \"./audio_files\"\n",
    "PATH_TO_TRANSCRIPT_FILES = \"./transcript_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Imports\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch, max_audio_length=160000):\n",
    "    audio_tensors = []\n",
    "    text_list = []\n",
    "\n",
    "    for audio, text in batch:\n",
    "        # Adjust audio length to max_audio_length\n",
    "        if audio.size(0) > max_audio_length:\n",
    "            audio = audio[:max_audio_length]\n",
    "        else:\n",
    "            audio = F.pad(audio, (0, max_audio_length - audio.size(0)))\n",
    "        \n",
    "        audio_tensors.append(audio)\n",
    "        text_list.append(text)\n",
    "\n",
    "    # Stack the audio tensors into a batch\n",
    "    audio_batch = torch.stack(audio_tensors)\n",
    "\n",
    "    # text_list is a list of strings\n",
    "    return audio_batch, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: TED-LIUM Dataset Setup\n",
    "class TEDLIUMDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transcript_dir, tokenizer, max_text_length=512, sample_rate=16000, test_size=10):\n",
    "        \"\"\"\n",
    "        Initialize dataset with paths to audio and transcript directories and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            audio_dir (str): Path to directory containing audio files.\n",
    "            transcript_dir (str): Path to directory containing transcript files.\n",
    "            tokenizer (transformers.AutoTokenizer): Tokenizer for text data.\n",
    "            max_text_length (int): Maximum length for text tokenization.\n",
    "            sample_rate (int): Desired sample rate for audio.\n",
    "            test_size (int): Number of samples to use for the test set.\n",
    "        \"\"\"\n",
    "        self.audio_files = sorted([\n",
    "            os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith(\".wav\")\n",
    "        ])\n",
    "        self.transcript_files = sorted([\n",
    "            os.path.join(transcript_dir, f) for f in os.listdir(transcript_dir) if f.endswith(\".stm\")\n",
    "        ])\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.test_size = test_size\n",
    "\n",
    "        # Split data into training and testing\n",
    "        self.train_audio_files = self.audio_files[:-self.test_size]  # All except last 10\n",
    "        self.test_audio_files = self.audio_files[-self.test_size:]   # Last 10\n",
    "        self.train_transcript_files = self.transcript_files[:-self.test_size]  # All except last 10\n",
    "        self.test_transcript_files = self.transcript_files[-self.test_size:]   # Last 10\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length should be the size of the training set (excluding test set entries)\n",
    "        return len(self.train_audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the training sample\n",
    "        audio_path = self.train_audio_files[idx]\n",
    "        transcript_path = self.train_transcript_files[idx]\n",
    "        \n",
    "        # Load audio file\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample to desired sample rate if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # Load transcript\n",
    "        with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "            transcript = f.read().strip()\n",
    "        \n",
    "        # Return waveform and transcript for training\n",
    "        return waveform.squeeze(0), transcript\n",
    "    \n",
    "    def get_test_samples(self):\n",
    "        \"\"\"Return a list of test samples (audio and corresponding transcript).\"\"\"\n",
    "        test_samples = []\n",
    "        for i in range(self.test_size):\n",
    "            audio_path = self.test_audio_files[i]\n",
    "            transcript_path = self.test_transcript_files[i]\n",
    "            \n",
    "            # Load audio file\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if sr != self.sample_rate:\n",
    "                waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "            # Load transcript\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                transcript = f.read().strip()\n",
    "            \n",
    "            # Append audio and transcript to test_samples\n",
    "            test_samples.append((waveform.squeeze(0), transcript))\n",
    "        \n",
    "        return test_samples\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = TEDLIUMDataset(audio_dir=PATH_TO_AUDIO_FILES, transcript_dir=PATH_TO_TRANSCRIPT_FILES, tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=lambda x: collate_fn(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define Training Components\n",
    "\n",
    "# Define a lower learning rate for pretrained encoders\n",
    "pretrained_lr = 1e-4  # Lower learning rate for BERT and Wav2Vec2\n",
    "training_lr = 1e-3  # Default learning rate for the rest of the model\n",
    "num_of_layers_to_unfreeze = 8\n",
    "\n",
    "# Initialize model components\n",
    "from transformers import Wav2Vec2Model, BertModel\n",
    "\n",
    "audio_encoder = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-960h').to(device)\n",
    "audio_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "text_encoder = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "text_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "for param in audio_encoder.parameters(): param.requires_grad = False\n",
    "for param in audio_encoder.encoder.layers[-num_of_layers_to_unfreeze:].parameters(): param.requires_grad = True\n",
    "for param in text_encoder.parameters(): param.requires_grad = False\n",
    "for param in text_encoder.encoder.layer[-num_of_layers_to_unfreeze:].parameters(): param.requires_grad = True\n",
    "\n",
    "projection_head_audio = ProjectionHead(embed_dim=1024, proj_dim=256).to(device)\n",
    "projection_head_text = ProjectionHead(embed_dim=768, proj_dim=256).to(device)\n",
    "\n",
    "contrastive_loss = ContrastiveLoss().to(device)\n",
    "droptoken = DropToken(drop_rate=0.5).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': audio_encoder.parameters(), 'lr': pretrained_lr},\n",
    "    {'params': text_encoder.parameters(), 'lr': pretrained_lr},\n",
    "    {'params': projection_head_audio.parameters(), 'lr': training_lr},\n",
    "    {'params': projection_head_text.parameters(), 'lr': training_lr},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 585/585 [05:19<00:00,  1.83batch/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Average Loss: 1.4239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 585/585 [04:23<00:00,  2.22batch/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Average Loss: 1.3990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 585/585 [04:24<00:00,  2.21batch/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Average Loss: 1.4123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 585/585 [04:34<00:00,  2.13batch/s, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Average Loss: 1.3952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 585/585 [04:35<00:00,  2.13batch/s, loss=1.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Average Loss: 1.3916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# Step 5: Training Loop with Progress Bars for Steps within Each Epoch\n",
    "def train(model_components, dataloader, optimizer, num_epochs=10):\n",
    "    (\n",
    "        audio_encoder,\n",
    "        text_encoder,\n",
    "        projection_head_audio,\n",
    "        projection_head_text,\n",
    "        contrastive_loss,\n",
    "        droptoken\n",
    "    ) = model_components\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # Initialize progress bar for batches within the current epoch\n",
    "        batch_progress = tqdm(\n",
    "            dataloader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "            unit=\"batch\",\n",
    "            leave=True  # Keeps the progress bar after the epoch completes\n",
    "        )\n",
    "        \n",
    "        accumulation_steps = 4\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        for i, (audio, text) in enumerate(batch_progress):\n",
    "            # Move audio to device; text remains on CPU\n",
    "            audio = audio.to(device)\n",
    "\n",
    "            # Audio processing with Wav2Vec2\n",
    "            audio_output = audio_encoder(audio)  # [batch, seq_len, 768]\n",
    "            audio_embeddings = audio_output.last_hidden_state  # Extract the hidden states\n",
    "            audio_embeddings = droptoken(audio_embeddings)     # Apply DropToken\n",
    "            audio_proj = projection_head_audio(audio_embeddings)  # Project to common space\n",
    "\n",
    "            # Text processing with BERT\n",
    "            tokenized_text = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            text_output = text_encoder(**tokenized_text)  # [batch, seq_len, 768]\n",
    "            text_embeddings = text_output.last_hidden_state  # Extract the hidden states\n",
    "            text_embeddings = droptoken(text_embeddings)     # Apply DropToken\n",
    "            text_proj = projection_head_text(text_embeddings)  # Project to common space\n",
    "\n",
    "            # Contrastive Loss\n",
    "            loss = contrastive_loss(audio_proj, text_proj)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Backpropagation\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)  # Apply the gradients\n",
    "                scaler.update()          # Update the scaler\n",
    "                optimizer.zero_grad()    # Reset gradients\n",
    "            # No need to call scaler.update() here since it's done after step()\n",
    "\n",
    "            # Update the progress bar's postfix with the current loss every 10 steps\n",
    "            if (i + 1) % 10 == 0:\n",
    "                batch_progress.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Define model components as a tuple\n",
    "model_components = (\n",
    "    audio_encoder,\n",
    "    text_encoder,\n",
    "    projection_head_audio,\n",
    "    projection_head_text,\n",
    "    contrastive_loss,\n",
    "    droptoken\n",
    ")\n",
    "\n",
    "# Start training\n",
    "num_epochs = 5\n",
    "train(model_components, dataloader, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained-5-1733197704.1865647.pth\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def save_model(model_components, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Save the state dictionaries of all model components and the optimizer.\n",
    "\n",
    "    Args:\n",
    "        model_components (tuple): Tuple containing all model components.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used during training.\n",
    "        epoch (int): The current epoch number.\n",
    "    \"\"\"\n",
    "    # Generate a unique filename for the saved model\n",
    "    path = f\"trained-{epoch}-{time.time()}.pth\"\n",
    "\n",
    "    # Unpack the components\n",
    "    (\n",
    "        audio_encoder,\n",
    "        text_encoder,\n",
    "        projection_head_audio,\n",
    "        projection_head_text,\n",
    "        contrastive_loss,\n",
    "        droptoken\n",
    "    ) = model_components\n",
    "\n",
    "    # Save state dictionaries\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'text_encoder_state_dict': text_encoder.state_dict(),\n",
    "        'audio_encoder_state_dict': audio_encoder.state_dict(),\n",
    "        'projection_head_audio_state_dict': projection_head_audio.state_dict(),\n",
    "        'projection_head_text_state_dict': projection_head_text.state_dict(),\n",
    "        'contrastive_loss_state_dict': contrastive_loss.state_dict(),\n",
    "        'droptoken_state_dict': droptoken.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "    torch.save({\n",
    "        'text_encoder_state_dict': text_encoder.state_dict(),\n",
    "        'projection_head_text_state_dict': projection_head_text.state_dict(),\n",
    "    }, \"text-our-vatt-\" + path)\n",
    "\n",
    "    torch.save({\n",
    "        'audio_encoder_state_dict': audio_encoder.state_dict(),\n",
    "        'projection_head_audio_state_dict': projection_head_audio.state_dict(),\n",
    "    }, \"audio-our-vatt-\" + path)\n",
    "\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "save_model(model_components, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component                          Parameters\n",
      "--------------------------------------------------\n",
      "Audio Tokenizer                   100,769,792\n",
      "Text Tokenizer                     56,702,976\n",
      "Projection Head (Audio)             2,623,744\n",
      "Projection Head (Text)              1,574,656\n",
      "Contrastive Loss                            0\n",
      "DropToken                                   0\n",
      "--------------------------------------------------\n",
      "Total Trainable Parameters        161,671,168\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model_components):\n",
    "    \"\"\"\n",
    "    Counts and prints the total number of trainable parameters for each model component.\n",
    "\n",
    "    Args:\n",
    "        model_components (tuple): A tuple of PyTorch model components.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    print(f\"{'Component':<30}{'Parameters':>15}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Iterate through each component in the tuple\n",
    "    component_names = [\n",
    "        \"Audio Tokenizer\",\n",
    "        \"Text Tokenizer\",\n",
    "        \"Projection Head (Audio)\",\n",
    "        \"Projection Head (Text)\",\n",
    "        \"Contrastive Loss\",\n",
    "        \"DropToken\"\n",
    "    ]\n",
    "\n",
    "    for name, component in zip(component_names, model_components):\n",
    "        if hasattr(component, \"parameters\"):\n",
    "            component_params = sum(p.numel() for p in component.parameters() if p.requires_grad)\n",
    "            total_params += component_params\n",
    "            print(f\"{name:<30}{component_params:>15,}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Total Trainable Parameters':<30}{total_params:>15,}\")\n",
    "    return total_params\n",
    "\n",
    "model_components = (\n",
    "    audio_encoder,\n",
    "    text_encoder,\n",
    "    projection_head_audio,\n",
    "    projection_head_text,\n",
    "    contrastive_loss,\n",
    "    droptoken\n",
    ")\n",
    "\n",
    "total_params = count_parameters(model_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ted_samples():\n",
    "    \"\"\"Loads test samples (audio, transcript) from the dataset\"\"\"\n",
    "    # Load the dataset\n",
    "    test_set = dataset.get_test_samples()\n",
    "    \n",
    "    sample_texts = []\n",
    "    sample_audio = []\n",
    "    \n",
    "    # Collect sample texts and corresponding audio\n",
    "    for audio, text in test_set:\n",
    "        sample_texts.append(text)\n",
    "        sample_audio.append(audio)\n",
    "    \n",
    "    return sample_audio, sample_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DropToken()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CLAP Model (from Huggingface)\n",
    "from transformers import AutoProcessor, ClapModel\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Your trained VATT Model (already loaded as model_components)\n",
    "audio_encoder, text_encoder, projection_head_audio, projection_head_text, contrastive_loss, droptoken = model_components\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "audio_encoder.eval()\n",
    "text_encoder.eval()\n",
    "projection_head_audio.eval()\n",
    "projection_head_text.eval()\n",
    "contrastive_loss.eval()\n",
    "droptoken.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract Summary for Our VATT Model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_summary_for_our_vatt(audio, text, model_components, tokenizer):\n",
    "    \"\"\"\n",
    "    Extracts a summary for the provided audio and text using the trained VATT model.\n",
    "    \n",
    "    Args:\n",
    "        audio (tensor): Audio tensor (waveform) for the input.\n",
    "        text (str): Corresponding transcript text for the input.\n",
    "        model_components (tuple): Tuple containing audio_encoder, text_encoder, projection_head_audio,\n",
    "                                  projection_head_text, contrastive_loss, droptoken.\n",
    "        tokenizer: Tokenizer used for text input.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted summary.\n",
    "    \"\"\"\n",
    "    audio_encoder, text_encoder, projection_head_audio, projection_head_text, contrastive_loss, droptoken = model_components\n",
    "    audio_encoder.to(device)\n",
    "    text_encoder.to(device)\n",
    "    projection_head_audio.to(device)\n",
    "    projection_head_text.to(device)\n",
    "    contrastive_loss.to(device)\n",
    "    droptoken.to(device)\n",
    "    \n",
    "    # Process audio input\n",
    "    audio = audio.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    audio_output = audio_encoder(audio)\n",
    "    audio_embeddings = audio_output.last_hidden_state\n",
    "    audio_embeddings = droptoken(audio_embeddings)\n",
    "    audio_proj = projection_head_audio(audio_embeddings)  # Project to common space\n",
    "\n",
    "    # Process text input\n",
    "    text_tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\").to(device)\n",
    "    text_output = text_encoder(**text_tokenized)\n",
    "    text_embeddings = text_output.last_hidden_state\n",
    "    text_embeddings = droptoken(text_embeddings)\n",
    "    text_proj = projection_head_text(text_embeddings)  # Project to common space\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    sim_score = cosine_similarity(audio_proj.cpu().detach().numpy(), text_proj.cpu().detach().numpy())\n",
    "    \n",
    "    top_k_indices = np.argsort(sim_score[0])[::-1][:5]  # Sorting to pick the top-k\n",
    "    \n",
    "    # Step 7: Construct the extractive summary\n",
    "    summary_sentences = [text[idx] for idx in top_k_indices]\n",
    "    summary = '\\n'.join(summary_sentences)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract Summary for CLAP Model\n",
    "def extract_summary_for_clap(audio, text, clap_model, clap_processor, max_text_length=512):\n",
    "    \"\"\"\n",
    "    Extracts a summary for the provided audio and text using the CLAP model.\n",
    "    \n",
    "    Args:\n",
    "        audio (tensor): Audio tensor (waveform) for the input.\n",
    "        text (str): Corresponding transcript text for the input.\n",
    "        clap_model: The pre-trained CLAP model.\n",
    "        clap_processor: The processor used for converting inputs to the model.\n",
    "        max_text_length (int): Maximum length of text sequences for padding/truncation.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process text (padding and truncation)\n",
    "    text_input = clap_processor(text=text, padding=True, truncation=True, max_length=max_text_length, return_tensors=\"pt\")\n",
    "\n",
    "    # Process audio (convert waveform to features)\n",
    "    audio_input = clap_processor(audios=audio, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Ensure that both text and audio inputs are of similar lengths (in case of mismatch)\n",
    "    # This could involve padding/truncating the audio to match the length of the text\n",
    "    # Here, we assume text and audio should align by some logic (this part may need model-specific adjustments)\n",
    "    if audio_input[\"input_features\"].shape[1] > text_input[\"input_ids\"].shape[1]:\n",
    "        audio_input[\"input_features\"] = audio_input[\"input_features\"][:, :text_input[\"input_ids\"].shape[1]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass the inputs through the CLAP model\n",
    "        outputs = clap_model(**text_input, input_features=audio_input[\"input_features\"])\n",
    "\n",
    "    # Similarity-based summary extraction (implement as per your logic)\n",
    "    sim_score = cosine_similarity(outputs.text_embeds.cpu().detach().numpy(), outputs.audio_embeds.cpu().detach().numpy())\n",
    "\n",
    "    top_k_indices = np.argsort(sim_score[0])[::-1][:5]  # Sorting to pick the top-k sentences based on similarity\n",
    "    \n",
    "    # Construct the extractive summary\n",
    "    summary_sentences = [text_input['input_ids'][0][idx] for idx in top_k_indices]\n",
    "    summary = ' '.join([clap_processor.decode([idx]) for idx in summary_sentences])  # Decode to text\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 2/10\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 5: Presenting Results for Human Evaluation\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def print_summaries(model_name, summaries):\n",
    "    \"\"\"\n",
    "    Prints summaries in a human-readable format for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model (e.g., \"VATT\" or \"CLAP\").\n",
    "        summaries (list of str): List of summaries to print.\n",
    "    \"\"\"\n",
    "    print(f\"==================== {model_name} Summaries ====================\")\n",
    "    for idx, summary in enumerate(summaries):\n",
    "        print(f\"\\nTest Case {idx+1}:\")\n",
    "        print(f\"Summary: {summary}\")\n",
    "    print(\"===============================================================\")\n",
    "\n",
    "\n",
    "sample_audio, sample_texts = load_ted_samples()\n",
    "\n",
    "# Prepare summaries for both models\n",
    "vatt_summaries = []\n",
    "clap_summaries = []\n",
    "i = 1\n",
    "\n",
    "# Extract summaries for the 5 test samples using both models\n",
    "for audio, text in zip(sample_audio, sample_texts):\n",
    "    print(f\"Summary {i}/{len(sample_texts)}\")\n",
    "    i+=1\n",
    "\n",
    "    vatt_summary = extract_summary_for_our_vatt(audio, text, model_components, tokenizer)\n",
    "    vatt_summaries.append(vatt_summary)\n",
    "\n",
    "    clap_summary = extract_summary_for_clap(audio, text, clap_model, clap_processor)\n",
    "    clap_summaries.append(clap_summary)\n",
    "\n",
    "print_summaries(\"VATT\", vatt_summaries)\n",
    "print_summaries(\"CLAP\", clap_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
