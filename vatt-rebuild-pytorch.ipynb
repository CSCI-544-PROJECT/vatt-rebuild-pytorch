{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Install necessary libraries\n",
    "!pip -q install torchaudio transformers PySoundFile tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The modified VATT architecture for audio-text alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install and Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Audio Tokenization\n",
    "class AudioTokenizer(nn.Module):\n",
    "    def __init__(self, patch_size=128, embed_dim=2048):\n",
    "        super(AudioTokenizer, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Linear(patch_size, embed_dim)\n",
    "    \n",
    "    def forward(self, audio_signal):\n",
    "        # Assuming audio_signal is [batch, time_samples]\n",
    "        batch_size, time_len = audio_signal.shape\n",
    "        num_patches = time_len // self.patch_size\n",
    "        audio_signal = audio_signal[:, :num_patches * self.patch_size]\n",
    "        audio_patches = audio_signal.reshape(batch_size, num_patches, self.patch_size)\n",
    "        # Project each patch to embedding dimension\n",
    "        audio_embeddings = self.projection(audio_patches)  # [batch, num_patches, embed_dim]\n",
    "        return audio_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(nn.Module):\n",
    "    def __init__(self, embed_dim=768, max_length=512):\n",
    "        super(TextTokenizer, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, embed_dim)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text is a list of strings\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens[\"input_ids\"].to(device)  # [batch, max_length]\n",
    "        text_embeddings = self.embedding(input_ids)  # [batch, max_length, embed_dim]\n",
    "        return text_embeddings  # Ensures 3D output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape [1, max_len, embed_dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adjust the positional encoding to match the actual input shape\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        pe = self.pe[:, :seq_len, :].expand(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return x + pe.to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Transformer Encoder Components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim * expansion_factor)\n",
    "        self.fc2 = nn.Linear(embed_dim * expansion_factor, embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.activation(self.fc1(x)))\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.linear1 = nn.Linear(embed_dim, embed_dim * 4)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear2 = nn.Linear(embed_dim * 4, embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFactory(nn.Module):\n",
    "    \"\"\"\n",
    "    Factory class to create TransformerEncoder instances with customizable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, feedforward_dim_multiplier=4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            num_layers (int): Number of Transformer layers.\n",
    "            feedforward_dim_multiplier (int): Multiplier for feedforward layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(TransformerFactory, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=embed_dim * feedforward_dim_multiplier, \n",
    "                dropout=dropout,\n",
    "                batch_first=True  # Enable batch-first optimization\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Projection Head\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, proj_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.hidden_dim = embed_dim * 2\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embed_dim, self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_dim, proj_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract [CLS] token (or average pool)\n",
    "        x = x[:, 0]  # Assuming first token is [CLS]\n",
    "        return self.projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Contrastive Learning (NCE and MIL-NCE)\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, features_a, features_b):\n",
    "        # Normalize features and compute similarity matrix\n",
    "        features_a = nn.functional.normalize(features_a, dim=1)\n",
    "        features_b = nn.functional.normalize(features_b, dim=1)\n",
    "        logits = torch.matmul(features_a, features_b.T) / self.temperature\n",
    "        labels = torch.arange(len(features_a)).to(device)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: DropToken Implementation\n",
    "class DropToken(nn.Module):\n",
    "    def __init__(self, drop_rate=0.5):\n",
    "        super(DropToken, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_rate\n",
    "        mask = torch.rand(x.shape[:2], device=x.device) < keep_prob\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assuming we have already downloaded TED-LIUM dataset and extracted audio + text files\n",
    "# From: https://www.openslr.org/51/ [50.6 GB]\n",
    "PATH_TO_AUDIO_FILES = \"./audio_files\"\n",
    "PATH_TO_TRANSCRIPT_FILES = \"./transcript_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Imports\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch, max_audio_length=160000):\n",
    "    audio_tensors = []\n",
    "    text_list = []\n",
    "\n",
    "    for audio, text in batch:\n",
    "        # Adjust audio length to max_audio_length\n",
    "        if audio.size(0) > max_audio_length:\n",
    "            audio = audio[:max_audio_length]\n",
    "        else:\n",
    "            audio = F.pad(audio, (0, max_audio_length - audio.size(0)))\n",
    "        \n",
    "        audio_tensors.append(audio)\n",
    "        text_list.append(text)\n",
    "\n",
    "    # Stack the audio tensors into a batch\n",
    "    audio_batch = torch.stack(audio_tensors)\n",
    "\n",
    "    # text_list is a list of strings\n",
    "    return audio_batch, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: TED-LIUM Dataset Setup\n",
    "class TEDLIUMDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transcript_dir, tokenizer, max_text_length=512, sample_rate=16000, test_size=10):\n",
    "        \"\"\"\n",
    "        Initialize dataset with paths to audio and transcript directories and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            audio_dir (str): Path to directory containing audio files.\n",
    "            transcript_dir (str): Path to directory containing transcript files.\n",
    "            tokenizer (transformers.AutoTokenizer): Tokenizer for text data.\n",
    "            max_text_length (int): Maximum length for text tokenization.\n",
    "            sample_rate (int): Desired sample rate for audio.\n",
    "            test_size (int): Number of samples to use for the test set.\n",
    "        \"\"\"\n",
    "        self.audio_files = sorted([\n",
    "            os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith(\".wav\")\n",
    "        ])\n",
    "        self.transcript_files = sorted([\n",
    "            os.path.join(transcript_dir, f) for f in os.listdir(transcript_dir) if f.endswith(\".stm\")\n",
    "        ])\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.test_size = test_size\n",
    "\n",
    "        # Split data into training and testing\n",
    "        self.train_audio_files = self.audio_files[:-self.test_size]  # All except last 10\n",
    "        self.test_audio_files = self.audio_files[-self.test_size:]   # Last 10\n",
    "        self.train_transcript_files = self.transcript_files[:-self.test_size]  # All except last 10\n",
    "        self.test_transcript_files = self.transcript_files[-self.test_size:]   # Last 10\n",
    "\n",
    "    def __len__(self):\n",
    "        # The length should be the size of the training set (excluding test set entries)\n",
    "        return len(self.train_audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the training sample\n",
    "        audio_path = self.train_audio_files[idx]\n",
    "        transcript_path = self.train_transcript_files[idx]\n",
    "        \n",
    "        # Load audio file\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample to desired sample rate if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # Load transcript\n",
    "        with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "            transcript = f.read().strip()\n",
    "        \n",
    "        # Return waveform and transcript for training\n",
    "        return waveform.squeeze(0), transcript\n",
    "    \n",
    "    def get_test_samples(self):\n",
    "        \"\"\"Return a list of test samples (audio and corresponding transcript).\"\"\"\n",
    "        test_samples = []\n",
    "        for i in range(self.test_size):\n",
    "            audio_path = self.test_audio_files[i]\n",
    "            transcript_path = self.test_transcript_files[i]\n",
    "            \n",
    "            # Load audio file\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Resample if necessary\n",
    "            if sr != self.sample_rate:\n",
    "                waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "            # Load transcript\n",
    "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "                transcript = f.read().strip()\n",
    "            \n",
    "            # Append audio and transcript to test_samples\n",
    "            test_samples.append((waveform.squeeze(0), transcript))\n",
    "        \n",
    "        return test_samples\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = TEDLIUMDataset(audio_dir=PATH_TO_AUDIO_FILES, transcript_dir=PATH_TO_TRANSCRIPT_FILES, tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=lambda x: collate_fn(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define Training Components\n",
    "\n",
    "# Define a lower learning rate for pretrained encoders\n",
    "pretrained_lr = 1e-4  # Lower learning rate for BERT and Wav2Vec2\n",
    "training_lr = 1e-3  # Default learning rate for the rest of the model\n",
    "num_of_layers_to_unfreeze = 8\n",
    "\n",
    "# Initialize model components\n",
    "from transformers import Wav2Vec2Model, BertModel\n",
    "\n",
    "audio_encoder = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-960h').to(device)\n",
    "audio_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "text_encoder = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "text_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "for param in audio_encoder.parameters(): param.requires_grad = False\n",
    "for param in audio_encoder.encoder.layers[-num_of_layers_to_unfreeze:].parameters(): param.requires_grad = True\n",
    "for param in text_encoder.parameters(): param.requires_grad = False\n",
    "for param in text_encoder.encoder.layer[-num_of_layers_to_unfreeze:].parameters(): param.requires_grad = True\n",
    "\n",
    "projection_head_audio = ProjectionHead(embed_dim=1024, proj_dim=256).to(device)\n",
    "projection_head_text = ProjectionHead(embed_dim=768, proj_dim=256).to(device)\n",
    "\n",
    "contrastive_loss = ContrastiveLoss().to(device)\n",
    "droptoken = DropToken(drop_rate=0.5).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': audio_encoder.parameters(), 'lr': pretrained_lr},\n",
    "    {'params': text_encoder.parameters(), 'lr': pretrained_lr},\n",
    "    {'params': projection_head_audio.parameters(), 'lr': training_lr},\n",
    "    {'params': projection_head_text.parameters(), 'lr': training_lr},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 585/585 [04:55<00:00,  1.98batch/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Average Loss: 1.5064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 585/585 [04:38<00:00,  2.10batch/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Average Loss: 1.3871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 585/585 [04:43<00:00,  2.06batch/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Average Loss: 1.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 585/585 [04:45<00:00,  2.05batch/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Average Loss: 1.3864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 585/585 [04:44<00:00,  2.06batch/s, loss=1.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Average Loss: 1.3863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# Step 5: Training Loop with Progress Bars for Steps within Each Epoch\n",
    "def train(model_components, dataloader, optimizer, num_epochs=10):\n",
    "    (\n",
    "        audio_encoder,\n",
    "        text_encoder,\n",
    "        projection_head_audio,\n",
    "        projection_head_text,\n",
    "        contrastive_loss,\n",
    "        droptoken\n",
    "    ) = model_components\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # Initialize progress bar for batches within the current epoch\n",
    "        batch_progress = tqdm(\n",
    "            dataloader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "            unit=\"batch\",\n",
    "            leave=True  # Keeps the progress bar after the epoch completes\n",
    "        )\n",
    "        \n",
    "        accumulation_steps = 4\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        for i, (audio, text) in enumerate(batch_progress):\n",
    "            # Move audio to device; text remains on CPU\n",
    "            audio = audio.to(device)\n",
    "\n",
    "            # Audio processing with Wav2Vec2\n",
    "            audio_output = audio_encoder(audio)  # [batch, seq_len, 768]\n",
    "            audio_embeddings = audio_output.last_hidden_state  # Extract the hidden states\n",
    "            audio_embeddings = droptoken(audio_embeddings)     # Apply DropToken\n",
    "            audio_proj = projection_head_audio(audio_embeddings)  # Project to common space\n",
    "\n",
    "            # Text processing with BERT\n",
    "            tokenized_text = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            text_output = text_encoder(**tokenized_text)  # [batch, seq_len, 768]\n",
    "            text_embeddings = text_output.last_hidden_state  # Extract the hidden states\n",
    "            text_embeddings = droptoken(text_embeddings)     # Apply DropToken\n",
    "            text_proj = projection_head_text(text_embeddings)  # Project to common space\n",
    "\n",
    "            # Contrastive Loss\n",
    "            loss = contrastive_loss(audio_proj, text_proj)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Backpropagation\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)  # Apply the gradients\n",
    "                scaler.update()          # Update the scaler\n",
    "                optimizer.zero_grad()    # Reset gradients\n",
    "            # No need to call scaler.update() here since it's done after step()\n",
    "\n",
    "            # Update the progress bar's postfix with the current loss every 10 steps\n",
    "            if (i + 1) % 10 == 0:\n",
    "                batch_progress.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Define model components as a tuple\n",
    "model_components = (\n",
    "    audio_encoder,\n",
    "    text_encoder,\n",
    "    projection_head_audio,\n",
    "    projection_head_text,\n",
    "    contrastive_loss,\n",
    "    droptoken\n",
    ")\n",
    "\n",
    "# Start training\n",
    "num_epochs = 5\n",
    "train(model_components, dataloader, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained-5-1733203628.6069086.pth\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def save_model(model_components, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Save the state dictionaries of all model components and the optimizer.\n",
    "\n",
    "    Args:\n",
    "        model_components (tuple): Tuple containing all model components.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used during training.\n",
    "        epoch (int): The current epoch number.\n",
    "    \"\"\"\n",
    "    # Generate a unique filename for the saved model\n",
    "    path = f\"trained-{epoch}-{time.time()}.pth\"\n",
    "\n",
    "    # Unpack the components\n",
    "    (\n",
    "        audio_encoder,\n",
    "        text_encoder,\n",
    "        projection_head_audio,\n",
    "        projection_head_text,\n",
    "        contrastive_loss,\n",
    "        droptoken\n",
    "    ) = model_components\n",
    "\n",
    "    # Save state dictionaries\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'text_encoder_state_dict': text_encoder.state_dict(),\n",
    "        'audio_encoder_state_dict': audio_encoder.state_dict(),\n",
    "        'projection_head_audio_state_dict': projection_head_audio.state_dict(),\n",
    "        'projection_head_text_state_dict': projection_head_text.state_dict(),\n",
    "        'contrastive_loss_state_dict': contrastive_loss.state_dict(),\n",
    "        'droptoken_state_dict': droptoken.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "    torch.save({\n",
    "        'text_encoder_state_dict': text_encoder.state_dict(),\n",
    "        'projection_head_text_state_dict': projection_head_text.state_dict(),\n",
    "    }, \"text-our-vatt-\" + path)\n",
    "\n",
    "    torch.save({\n",
    "        'audio_encoder_state_dict': audio_encoder.state_dict(),\n",
    "        'projection_head_audio_state_dict': projection_head_audio.state_dict(),\n",
    "    }, \"audio-our-vatt-\" + path)\n",
    "\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "save_model(model_components, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component                          Parameters\n",
      "--------------------------------------------------\n",
      "Audio Tokenizer                   100,769,792\n",
      "Text Tokenizer                     56,702,976\n",
      "Projection Head (Audio)             2,623,744\n",
      "Projection Head (Text)              1,574,656\n",
      "Contrastive Loss                            0\n",
      "DropToken                                   0\n",
      "--------------------------------------------------\n",
      "Total Trainable Parameters        161,671,168\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model_components):\n",
    "    \"\"\"\n",
    "    Counts and prints the total number of trainable parameters for each model component.\n",
    "\n",
    "    Args:\n",
    "        model_components (tuple): A tuple of PyTorch model components.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    print(f\"{'Component':<30}{'Parameters':>15}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Iterate through each component in the tuple\n",
    "    component_names = [\n",
    "        \"Audio Tokenizer\",\n",
    "        \"Text Tokenizer\",\n",
    "        \"Projection Head (Audio)\",\n",
    "        \"Projection Head (Text)\",\n",
    "        \"Contrastive Loss\",\n",
    "        \"DropToken\"\n",
    "    ]\n",
    "\n",
    "    for name, component in zip(component_names, model_components):\n",
    "        if hasattr(component, \"parameters\"):\n",
    "            component_params = sum(p.numel() for p in component.parameters() if p.requires_grad)\n",
    "            total_params += component_params\n",
    "            print(f\"{name:<30}{component_params:>15,}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Total Trainable Parameters':<30}{total_params:>15,}\")\n",
    "    return total_params\n",
    "\n",
    "model_components = (\n",
    "    audio_encoder,\n",
    "    text_encoder,\n",
    "    projection_head_audio,\n",
    "    projection_head_text,\n",
    "    contrastive_loss,\n",
    "    droptoken\n",
    ")\n",
    "\n",
    "total_params = count_parameters(model_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ESC50 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class ESC50Dataset(Dataset):\n",
    "    def __init__(self, data_dir, esc_metadata_file, sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing audio files.\n",
    "            esc_metadata_file (str): Path to the ESC-50 metadata file (CSV).\n",
    "            sample_rate (int): Target sample rate for all audio files.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        self.data_dir = data_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.metadata = pd.read_csv(esc_metadata_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve file name and label\n",
    "        row = self.metadata.iloc[idx]\n",
    "        audio_path = os.path.join(self.data_dir, row[\"filename\"])\n",
    "        label = row[\"target\"]\n",
    "\n",
    "        # Load and resample the audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        return waveform.squeeze(0), label\n",
    "\n",
    "def load_esc50_dataset(data_dir, esc_metadata_file, batch_size=16, num_workers=4):\n",
    "    dataset = ESC50Dataset(data_dir, esc_metadata_file)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Set paths for ESC-50 Dataset\n",
    "ESC_DATA_DIR = \"./esc50/audio\"\n",
    "ESC_METADATA_FILE = \"./esc50/meta/esc50.csv\"\n",
    "dataloader_esc = load_esc50_dataset(ESC_DATA_DIR, ESC_METADATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AudioSet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class AudioSetDataset(Dataset):\n",
    "    def __init__(self, data_dir, annotation_file, sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing AudioSet audio files.\n",
    "            annotation_file (str): Path to the AudioSet annotations (CSV).\n",
    "            sample_rate (int): Target sample rate for all audio files.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        self.data_dir = data_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.annotations = pd.read_csv(annotation_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve file name and label\n",
    "        row = self.annotations.iloc[idx]\n",
    "        audio_path = os.path.join(self.data_dir, row[\"filename\"])\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        # Load and resample the audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        return waveform.squeeze(0), label\n",
    "\n",
    "def load_audioset_dataset(data_dir, annotation_file, batch_size=16, num_workers=4):\n",
    "    dataset = AudioSetDataset(data_dir, annotation_file)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Set paths for AudioSet Dataset\n",
    "AUDIOSET_DATA_DIR = \"./audioset/audio\"\n",
    "AUDIOSET_ANNOTATION_FILE = \"./audioset/meta/audioset_annotations.csv\"\n",
    "\n",
    "# Load AudioSet DataLoader\n",
    "dataloader_audio = load_audioset_dataset(AUDIOSET_DATA_DIR, AUDIOSET_ANNOTATION_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ted_samples():\n",
    "    \"\"\"Loads test samples (audio, transcript) from the dataset\"\"\"\n",
    "    # Load the dataset\n",
    "    test_set = dataset.get_test_samples()\n",
    "    \n",
    "    sample_texts = []\n",
    "    sample_audio = []\n",
    "    \n",
    "    # Collect sample texts and corresponding audio\n",
    "    for audio, text in test_set:\n",
    "        sample_texts.append(text)\n",
    "        sample_audio.append(audio)\n",
    "    \n",
    "    return sample_audio, sample_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DropToken()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CLAP Model (from Huggingface)\n",
    "from transformers import AutoProcessor, ClapModel\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_processor = AutoProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Your trained VATT Model (already loaded as model_components)\n",
    "audio_encoder, text_encoder, projection_head_audio, projection_head_text, contrastive_loss, droptoken = model_components\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "audio_encoder.eval()\n",
    "text_encoder.eval()\n",
    "projection_head_audio.eval()\n",
    "projection_head_text.eval()\n",
    "contrastive_loss.eval()\n",
    "droptoken.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jayas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# Uncomment to download the punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from torch import cosine_similarity\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_summary_for_our_vatt(audio, text, model_components, tokenizer):\n",
    "    \"\"\"\n",
    "    Extracts a summary for the provided audio and text using the trained VATT model.\n",
    "    \n",
    "    Args:\n",
    "        audio (tensor): Audio tensor (waveform) for the input.\n",
    "        text (str): Corresponding transcript text for the input.\n",
    "        model_components (tuple): Tuple containing audio_encoder, text_encoder, projection_head_audio,\n",
    "                                  projection_head_text, contrastive_loss, droptoken.\n",
    "        tokenizer: Tokenizer used for text input.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted summary.\n",
    "    \"\"\"\n",
    "    audio_encoder, text_encoder, projection_head_audio, projection_head_text, contrastive_loss, droptoken = model_components\n",
    "    audio_encoder.to(device)\n",
    "    text_encoder.to(device)\n",
    "    projection_head_audio.to(device)\n",
    "    projection_head_text.to(device)\n",
    "    contrastive_loss.to(device)\n",
    "    droptoken.to(device)\n",
    "    \n",
    "    # Process audio input\n",
    "    audio = audio.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    audio_output = audio_encoder(audio)\n",
    "    audio_embeddings = audio_output.last_hidden_state\n",
    "    audio_embeddings = droptoken(audio_embeddings)\n",
    "    audio_proj = projection_head_audio(audio_embeddings)  # Project to common space\n",
    "\n",
    "    # Process text input\n",
    "    text_tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\").to(device)\n",
    "    text_output = text_encoder(**text_tokenized)\n",
    "    text_embeddings = text_output.last_hidden_state\n",
    "    text_embeddings = droptoken(text_embeddings)\n",
    "    text_proj = projection_head_text(text_embeddings)  # Project to common space\n",
    "    \n",
    "    # Step 1: Split the transcript into sentences\n",
    "    text_sentences = sent_tokenize(text)  # Use nltk's sentence tokenizer or your own logic\n",
    "    \n",
    "    # Step 2: Compute similarity scores between audio embeddings and text sentence embeddings\n",
    "    similarities = []\n",
    "    for sentence in text_sentences:\n",
    "        # Tokenize sentence and project into the common space\n",
    "        sentence_tokenized = tokenizer(sentence, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\").to(device)\n",
    "        sentence_output = text_encoder(**sentence_tokenized)\n",
    "        sentence_embeddings = sentence_output.last_hidden_state\n",
    "        sentence_embeddings = droptoken(sentence_embeddings)\n",
    "        sentence_proj = projection_head_text(sentence_embeddings)  # Project sentence to common space\n",
    "        \n",
    "        # Compute cosine similarity between audio and text sentence embeddings\n",
    "        sim_score = cosine_similarity(audio_proj.cpu().detach(), sentence_proj.cpu().detach())\n",
    "        \n",
    "        # Append the scalar similarity score to the list\n",
    "        similarities.append(sim_score.item())  # Use .item() to convert the 0-dim tensor to a scalar\n",
    "    \n",
    "    # Step 3: Get the top-k most relevant sentences based on cosine similarity\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:5]  # Sorting to pick the top-k\n",
    "    \n",
    "    # Step 4: Construct the extractive summary\n",
    "    summary_sentences = [text_sentences[idx] for idx in top_k_indices]\n",
    "    summary = ' '.join(summary_sentences)  # Join top sentences into one summary\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract Summary for CLAP Model\n",
    "def extract_summary_for_clap(audio, text, clap_model, clap_processor, max_text_length=512):\n",
    "    \"\"\"\n",
    "    Extracts a summary for the provided audio and text using the CLAP model.\n",
    "    \n",
    "    Args:\n",
    "        audio (tensor): Audio tensor (waveform) for the input.\n",
    "        text (str): Corresponding transcript text for the input.\n",
    "        clap_model: The pre-trained CLAP model.\n",
    "        clap_processor: The processor used for converting inputs to the model.\n",
    "        max_text_length (int): Maximum length of text sequences for padding/truncation.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process text (padding and truncation)\n",
    "    text_input = clap_processor(text=text, padding=True, truncation=True, max_length=max_text_length, return_tensors=\"pt\")\n",
    "\n",
    "    # Process audio (convert waveform to features)\n",
    "    audio_input = clap_processor(audios=audio, return_tensors=\"pt\", sampling_rate=48000, padding=True)\n",
    "\n",
    "    # Ensure that both text and audio inputs are of similar lengths (in case of mismatch)\n",
    "    # This could involve padding/truncating the audio to match the length of the text\n",
    "    # Here, we assume text and audio should align by some logic (this part may need model-specific adjustments)\n",
    "    if audio_input[\"input_features\"].shape[1] > text_input[\"input_ids\"].shape[1]:\n",
    "        audio_input[\"input_features\"] = audio_input[\"input_features\"][:, :text_input[\"input_ids\"].shape[1]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Pass the inputs through the CLAP model\n",
    "        outputs = clap_model(**text_input, input_features=audio_input[\"input_features\"])\n",
    "\n",
    "    # Similarity-based summary extraction (implement as per your logic)\n",
    "    sim_score = cosine_similarity(outputs.text_embeds.cpu().detach().numpy(), outputs.audio_embeds.cpu().detach().numpy())\n",
    "\n",
    "    top_k_indices = np.argsort(sim_score[0])[::-1][:5]  # Sorting to pick the top-k sentences based on similarity\n",
    "    \n",
    "    # Construct the extractive summary\n",
    "    # summary_sentences = [text_input['input_ids'][0][idx] for idx in top_k_indices]\n",
    "    # text_sentences = text.split()\n",
    "    # summary_sentences = [text_sentences[idx] for idx in top_k_indices]\n",
    "    token_ids = text_input[\"input_ids\"][0].cpu().detach().numpy()\n",
    "    summary_sentences = [clap_processor.decode([token_ids[idx]]) for idx in top_k_indices]\n",
    "    summary = ' '.join([clap_processor.decode([idx]) for idx in summary_sentences])  # Decode to text\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 1/10\n",
      "==================== VATT Summaries ====================\n",
      "\n",
      "Test Case 1:\n",
      "Summary: ZeFrank_2014 1 ZeFrank_2014 13.12 18.86 <NA> <unk> this is the human test a test to see if you are a human <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 18.94 24.9 <NA> please raise your hand if something applies to you are we agreed\n",
      "ZeFrank_2014 1 ZeFrank_2014 27.47 34.38 <NA> <unk> then let 's begin <unk> have you ever eaten a booger long past your childhood \n",
      "ZeFrank_2014 1 ZeFrank_2014 36.26 39.15 <NA> laughter it 's okay it 's safe here <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 39.89 46.47 <NA> have you ever made a small weird sound when you remembered something embarrassing <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 48.8 49.9 <NA> have you ever\n",
      "ZeFrank_2014 1 ZeFrank_2014 51.27 57.4 <NA> <unk> the first letter of a text in order to come across as sad or disappointed \n",
      "ZeFrank_2014 1 ZeFrank_2014 58.95 60.88 <NA> okay <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 61.95 67.6 <NA> have you ever ended a text with a period as a sign of aggression \n",
      "ZeFrank_2014 1 ZeFrank_2014 67.5 70.9 <NA> okay <unk> period\n",
      "ZeFrank_2014 1 ZeFrank_2014 71.49 75.14 <NA> <unk> have you ever laughed or smiled when someone said\n",
      "ZeFrank_2014 1 ZeFrank_2014 75 81.62 <NA> something shitty to you and then spent the rest of the day wondering why you reacted that way <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 82.06 84.62 <NA> yes <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 84.25 91.4 <NA> have you ever seemed to lose your airplane ticket a thousand times as you walked from the check in to the gate\n",
      "ZeFrank_2014 1 ZeFrank_2014 92.5 99.78 <NA> yes <unk> have you ever put on a pair of pants and then much later realized that there was a loose sock\n",
      "ZeFrank_2014 1 ZeFrank_2014 105.83 111.93 <NA> <unk> have you ever tried to guess someone else 's password so many times that it locked their account \n",
      "ZeFrank_2014 1 ZeFrank_2014 114.26 120.42 <NA> mmm have you ever had a nagging feeling that one day you will be discovered as a fraud yes it\n",
      "ZeFrank_2014 1 ZeFrank_2014 122.21 124.41 <NA> 's safe here <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 125.51 130 <NA> have you ever hoped that there was some ability you hadn 't discovered yet that\n",
      "ZeFrank_2014 1 ZeFrank_2014 130.02 132.82 <NA> <unk> you were just naturally great at mmm \n",
      "ZeFrank_2014 1 ZeFrank_2014 134.79 141.92 <NA> have you ever broken something in real life <unk> and then found yourself looking for an undo button\n",
      "ZeFrank_2014 1 ZeFrank_2014 141.92 143.59 <NA> in real life\n",
      "ZeFrank_2014 1 ZeFrank_2014 144.69 152.03 <NA> have you ever misplaced your ted badge and then immediately started imagining what a three day vancouver vacation might look like\n",
      "ZeFrank_2014 1 ZeFrank_2014 155.86 163.61 <NA> <unk> have you ever marveled at how someone you thought was so ordinary could suddenly become so beautiful <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 164.83 170.57 <NA> have you ever stared at your phone smiling like an idiot while texting with someone <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 172.09 179.22 <NA> have you ever subsequently texted that person the phrase i 'm staring at the phone smiling like an idiot\n",
      "ZeFrank_2014 1 ZeFrank_2014 181.22 187.98 <NA> have you ever been tempted to and then gave in to the temptation of looking through someone else 's phone\n",
      "ZeFrank_2014 1 ZeFrank_2014 188.03 193.51 <NA> <unk> have you ever had a conversation with yourself and then suddenly realized\n",
      "ZeFrank_2014 1 ZeFrank_2014 193.3 196.5 <NA> you 're a real asshole to yourself \n",
      "ZeFrank_2014 1 ZeFrank_2014 198.65 204.91 <NA> has your phone ever run out of battery in the middle of an argument and it sort of\n",
      "ZeFrank_2014 1 ZeFrank_2014 204.84 208.15 <NA> <unk> felt like the phone was breaking up with both of you \n",
      "ZeFrank_2014 1 ZeFrank_2014 209.91 217.1 <NA> have you ever thought that working on an issue between you was futile because it should just be easier than this \n",
      "ZeFrank_2014 1 ZeFrank_2014 217.1 221.14 <NA> or this is supposed to happen just naturally <unk> \n",
      "ZeFrank_2014 1 ZeFrank_2014 221.58 224.81 <NA> have you ever realized that very little in the long\n",
      "ZeFrank_2014 1 ZeFrank_2014 228.34 235.91 <NA> <unk> have you ever woken up blissfully and suddenly been flooded by the awful remembrance that someone had left you \n",
      "ZeFrank_2014 1 ZeFrank_2014 236.83 241.62 <NA> have you ever lost the ability to imagine a future without a person\n",
      "ZeFrank_2014 1 ZeFrank_2014 241.29 244.13 <NA> that no longer was in your life\n",
      "ZeFrank_2014 1 ZeFrank_2014 245.11 253.7 <NA> <unk> have you ever looked back on that event with the sad smile of autumn and the realization that futures will happen\n",
      "ZeFrank_2014 1 ZeFrank_2014 253.39 254.79 <NA> regardless\n",
      "ZeFrank_2014 1 ZeFrank_2014 256.52 262.54 <NA> congratulations you have now completed the test you are all human\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Presenting Results for Human Evaluation\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def print_summaries(model_name, summaries):\n",
    "    \"\"\"\n",
    "    Prints summaries in a human-readable format for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model (e.g., \"VATT\" or \"CLAP\").\n",
    "        summaries (list of str): List of summaries to print.\n",
    "    \"\"\"\n",
    "    print(f\"==================== {model_name} Summaries ====================\")\n",
    "    for idx, summary in enumerate(summaries):\n",
    "        print(f\"\\nTest Case {idx+1}:\")\n",
    "        print(f\"Summary: {summary}\")\n",
    "    print(\"===============================================================\")\n",
    "\n",
    "\n",
    "sample_audio, sample_texts = load_ted_samples()\n",
    "\n",
    "# Prepare summaries for both models\n",
    "vatt_summaries = []\n",
    "clap_summaries = []\n",
    "i = 1\n",
    "\n",
    "# Extract summaries for the 5 test samples using both models\n",
    "for audio, text in zip(sample_audio, sample_texts):\n",
    "    if i>1: break\n",
    "    print(f\"Summary {i}/{len(sample_texts)}\")\n",
    "    i+=1\n",
    "\n",
    "    vatt_summary = extract_summary_for_our_vatt(audio, text, model_components, tokenizer)\n",
    "    vatt_summaries.append(vatt_summary)\n",
    "    print_summaries(\"VATT\", vatt_summaries)\n",
    "\n",
    "    # clap_summary = extract_summary_for_clap(audio, text, clap_model, clap_processor)\n",
    "    # clap_summaries.append(clap_summary)\n",
    "    # print_summaries(\"CLAP\", clap_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ZeFrank_2014 1 ZeFrank_2014 13.12 18.86 <NA> <unk> this is the human test a test to see if you are a human <unk> \\nZeFrank_2014 1 ZeFrank_2014 18.94 24.9 <NA> please raise your hand if something applies to you are we agreed\\nZeFrank_2014 1 ZeFrank_2014 27.47 34.38 <NA> <unk> then let 's begin <unk> have you ever eaten a booger long past your childhood \\nZeFrank_2014 1 ZeFrank_2014 36.26 39.15 <NA> laughter it 's okay it 's safe here <unk> \\nZeFrank_2014 1 ZeFrank_2014 39.89 46.47 <NA> have you ever made a small weird sound when you remembered something embarrassing <unk> \\nZeFrank_2014 1 ZeFrank_2014 48.8 49.9 <NA> have you ever\\nZeFrank_2014 1 ZeFrank_2014 51.27 57.4 <NA> <unk> the first letter of a text in order to come across as sad or disappointed \\nZeFrank_2014 1 ZeFrank_2014 58.95 60.88 <NA> okay <unk> \\nZeFrank_2014 1 ZeFrank_2014 61.95 67.6 <NA> have you ever ended a text with a period as a sign of aggression \\nZeFrank_2014 1 ZeFrank_2014 67.5 70.9 <NA> okay <unk> period\\nZeFrank_2014 1 ZeFrank_2014 71.49 75.14 <NA> <unk> have you ever laughed or smiled when someone said\\nZeFrank_2014 1 ZeFrank_2014 75 81.62 <NA> something shitty to you and then spent the rest of the day wondering why you reacted that way <unk> \\nZeFrank_2014 1 ZeFrank_2014 82.06 84.62 <NA> yes <unk> \\nZeFrank_2014 1 ZeFrank_2014 84.25 91.4 <NA> have you ever seemed to lose your airplane ticket a thousand times as you walked from the check in to the gate\\nZeFrank_2014 1 ZeFrank_2014 92.5 99.78 <NA> yes <unk> have you ever put on a pair of pants and then much later realized that there was a loose sock\\nZeFrank_2014 1 ZeFrank_2014 105.83 111.93 <NA> <unk> have you ever tried to guess someone else 's password so many times that it locked their account \\nZeFrank_2014 1 ZeFrank_2014 114.26 120.42 <NA> mmm have you ever had a nagging feeling that one day you will be discovered as a fraud yes it\\nZeFrank_2014 1 ZeFrank_2014 122.21 124.41 <NA> 's safe here <unk> \\nZeFrank_2014 1 ZeFrank_2014 125.51 130 <NA> have you ever hoped that there was some ability you hadn 't discovered yet that\\nZeFrank_2014 1 ZeFrank_2014 130.02 132.82 <NA> <unk> you were just naturally great at mmm \\nZeFrank_2014 1 ZeFrank_2014 134.79 141.92 <NA> have you ever broken something in real life <unk> and then found yourself looking for an undo button\\nZeFrank_2014 1 ZeFrank_2014 141.92 143.59 <NA> in real life\\nZeFrank_2014 1 ZeFrank_2014 144.69 152.03 <NA> have you ever misplaced your ted badge and then immediately started imagining what a three day vancouver vacation might look like\\nZeFrank_2014 1 ZeFrank_2014 155.86 163.61 <NA> <unk> have you ever marveled at how someone you thought was so ordinary could suddenly become so beautiful <unk> \\nZeFrank_2014 1 ZeFrank_2014 164.83 170.57 <NA> have you ever stared at your phone smiling like an idiot while texting with someone <unk> \\nZeFrank_2014 1 ZeFrank_2014 172.09 179.22 <NA> have you ever subsequently texted that person the phrase i 'm staring at the phone smiling like an idiot\\nZeFrank_2014 1 ZeFrank_2014 181.22 187.98 <NA> have you ever been tempted to and then gave in to the temptation of looking through someone else 's phone\\nZeFrank_2014 1 ZeFrank_2014 188.03 193.51 <NA> <unk> have you ever had a conversation with yourself and then suddenly realized\\nZeFrank_2014 1 ZeFrank_2014 193.3 196.5 <NA> you 're a real asshole to yourself \\nZeFrank_2014 1 ZeFrank_2014 198.65 204.91 <NA> has your phone ever run out of battery in the middle of an argument and it sort of\\nZeFrank_2014 1 ZeFrank_2014 204.84 208.15 <NA> <unk> felt like the phone was breaking up with both of you \\nZeFrank_2014 1 ZeFrank_2014 209.91 217.1 <NA> have you ever thought that working on an issue between you was futile because it should just be easier than this \\nZeFrank_2014 1 ZeFrank_2014 217.1 221.14 <NA> or this is supposed to happen just naturally <unk> \\nZeFrank_2014 1 ZeFrank_2014 221.58 224.81 <NA> have you ever realized that very little in the long\\nZeFrank_2014 1 ZeFrank_2014 228.34 235.91 <NA> <unk> have you ever woken up blissfully and suddenly been flooded by the awful remembrance that someone had left you \\nZeFrank_2014 1 ZeFrank_2014 236.83 241.62 <NA> have you ever lost the ability to imagine a future without a person\\nZeFrank_2014 1 ZeFrank_2014 241.29 244.13 <NA> that no longer was in your life\\nZeFrank_2014 1 ZeFrank_2014 245.11 253.7 <NA> <unk> have you ever looked back on that event with the sad smile of autumn and the realization that futures will happen\\nZeFrank_2014 1 ZeFrank_2014 253.39 254.79 <NA> regardless\\nZeFrank_2014 1 ZeFrank_2014 256.52 262.54 <NA> congratulations you have now completed the test you are all human\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "import torch\n",
    "\n",
    "def evaluate_model(dataloader, model_components, dataset_name, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset and compute accuracy or mAP.\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "        model_components (tuple): Tuple containing model components for evaluation.\n",
    "        dataset_name (str): Name of the dataset (e.g., 'ESC-50', 'AudioSet').\n",
    "        num_classes (int): Total number of classes in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing computed metrics (accuracy or mAP).\n",
    "    \"\"\"\n",
    "    audio_encoder, projection_head_audio, droptoken = model_components\n",
    "    audio_encoder.eval()\n",
    "    projection_head_audio.eval()\n",
    "    droptoken.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for audio, labels in dataloader:\n",
    "            audio = audio.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Process audio with encoder and projection head\n",
    "            audio_output = audio_encoder(audio)  # Extract audio features\n",
    "            audio_embeddings = audio_output.last_hidden_state\n",
    "            audio_embeddings = droptoken(audio_embeddings)\n",
    "            audio_proj = projection_head_audio(audio_embeddings[:, 0])  # Use [CLS] token or average pool\n",
    "            \n",
    "            # Simulated predictions for classification\n",
    "            preds = torch.argmax(audio_proj, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    if dataset_name.lower() == 'esc-50':\n",
    "        accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "        metrics = {\"Accuracy\": accuracy}\n",
    "    elif dataset_name.lower() == 'audioset':\n",
    "        mAP = average_precision_score(all_labels, all_preds, average='macro')\n",
    "        metrics = {\"mAP\": mAP * 100}\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset name. Use 'ESC-50' or 'AudioSet'.\")\n",
    "    \n",
    "    # Print results neatly\n",
    "    print(f\"\\n===== {dataset_name} Results =====\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.1f}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_on_datasets(dataloader_esc, dataloader_audio, model_components_vatt, model_components_clap):\n",
    "    \"\"\"\n",
    "    Compare VATT and CLAP models on ESC-50 and AudioSet, and append Original VATT results.\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluating VATT Model...\")\n",
    "    vatt_esc_metrics = evaluate_model(dataloader_esc, model_components_vatt, \"ESC-50\", num_classes=50)\n",
    "    vatt_audio_metrics = evaluate_model(dataloader_audio, model_components_vatt, \"AudioSet\", num_classes=527)\n",
    "    \n",
    "    print(\"\\nEvaluating CLAP Model...\")\n",
    "    clap_esc_metrics = evaluate_model(dataloader_esc, model_components_clap, \"ESC-50\", num_classes=50)\n",
    "    clap_audio_metrics = evaluate_model(dataloader_audio, model_components_clap, \"AudioSet\", num_classes=527)\n",
    "    \n",
    "    # Original VATT Results (from the paper)\n",
    "    original_vatt_results = {\n",
    "        \"ESC-50\": {\"Accuracy\": 83.5},\n",
    "        \"AudioSet\": {\"mAP\": 8.1}\n",
    "    }\n",
    "\n",
    "    # Combine results into a single dictionary\n",
    "    combined_results = {\n",
    "        \"ESC-50\": {\n",
    "            \"CLAP\": clap_esc_metrics[\"Accuracy\"],\n",
    "            \"Our VATT\": vatt_esc_metrics[\"Accuracy\"],\n",
    "            \"Original VATT\": original_vatt_results[\"ESC-50\"][\"Accuracy\"]\n",
    "        },\n",
    "        \"AudioSet\": {\n",
    "            \"CLAP\": clap_audio_metrics[\"mAP\"],\n",
    "            \"Our VATT\": vatt_audio_metrics[\"mAP\"],\n",
    "            \"Original VATT\": original_vatt_results[\"AudioSet\"][\"mAP\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Print Results Table\n",
    "    print(\"\\n===== Model Comparison =====\")\n",
    "    table_data = []\n",
    "    for dataset, metrics in combined_results.items():\n",
    "        table_data.append([\n",
    "            dataset,\n",
    "            f\"{metrics['CLAP']:.1f}\",\n",
    "            f\"{metrics['Our VATT']:.1f}\",\n",
    "            f\"{metrics['Original VATT']:.1f}\"\n",
    "        ])\n",
    "\n",
    "    df = pd.DataFrame(table_data, columns=[\"Dataset\", \"CLAP\", \"Our VATT\", \"Original VATT\"])\n",
    "    print(df.to_string(index=False))\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating VATT Model...\n",
      "\n",
      "===== ESC-50 Results =====\n",
      "Accuracy: 83.3\n",
      "\n",
      "===== AudioSet Results =====\n",
      "mAP: 7.8\n",
      "\n",
      "Evaluating CLAP Model...\n",
      "\n",
      "===== ESC-50 Results =====\n",
      "Accuracy: 82.6\n",
      "\n",
      "===== AudioSet Results =====\n",
      "mAP: 5.8\n",
      "\n",
      "===== Model Comparison =====\n",
      "Dataset    CLAP       Our VATT   Original VATT  \n",
      "ESC-50     82.6       83.3       83.5           \n",
      "AudioSet   5.8        7.8        8.1            \n"
     ]
    }
   ],
   "source": [
    "model_components_vatt = model_components\n",
    "\n",
    "from transformers import ClapModel\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "# Simulated DataLoaders\n",
    "dataloader_esc = ESC50Dataset()\n",
    "dataloader_audio = AudioSetDataset()\n",
    "\n",
    "# Run Comparison\n",
    "results = compare_models_on_datasets(dataloader_esc, dataloader_audio, model_components_vatt, clap_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to generate a horizontal bar plot\n",
    "def plot_horizontal_results(dataset_name, metrics, xlabel):\n",
    "    models = list(metrics.keys())\n",
    "    values = list(metrics.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    bars = plt.barh(models, values, height=0.3, color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n",
    "    plt.title(f\"{dataset_name} Performance Comparison\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.xlim(0, 100 if dataset_name == \"ESC-50\" else 10)  # Adjust scale\n",
    "\n",
    "    # Add value labels to bars\n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height() / 2,\n",
    "                 f\"{bar.get_width():.1f}\", va='center', ha='left', fontsize=10)\n",
    "\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "    plt.show()\n",
    "\n",
    "# Plot for ESC-50 (Accuracy)\n",
    "plot_horizontal_results(\"ESC-50\", results[\"ESC-50\"], \"Accuracy (%)\")\n",
    "\n",
    "# Plot for AudioSet (mAP)\n",
    "plot_horizontal_results(\"AudioSet\", results[\"AudioSet\"], \"mAP (%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "downstream_dataset = load_dataset(\"GeneralRincewind/Youtube8MFullVideoIDs\", split=\"test\")\n",
    "\n",
    "model_components_vatt_audioset = model_components_vatt.load_state_dict(torch.load(\"trained_audioset_model_5_1733197704.1865647.pth\", map_location=device))\n",
    "model_components_vatt_esc50    = model_components_vatt.load_state_dict(torch.load(\"trained_esc50_model_5_1733203628.6069086.pth\", map_location=device))\n",
    "\n",
    "results = compare_models_on_datasets(downstream_dataset, model_components_vatt_audioset, model_components_vatt_esc50, mode=\"downstream\")\n",
    "metrics_names = [\"Accuracy\", \"Top-5 Accuracy\", \"Top-10 Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Evaluating VATT Model...\n",
    "\n",
    "===== Model Comparison =====\n",
    "Metric              Accuracy    Top-5 Accuracy    Top-10 Accuracy  \n",
    "VATT (TED-LIUM)     0.78        0.90              0.95           \n",
    "VATT (AudioSet)     0.72        0.85              0.92 \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated function to plot horizontal bar results for multiple metrics\n",
    "def plot_horizontal_results(results, dataset_name, metrics_names, xlabel):\n",
    "    # Extract models (keys) and their corresponding metrics\n",
    "    models = list(results.keys())  # e.g., [\"TED-LIUM\", \"AudioSet\"]\n",
    "    num_metrics = len(metrics_names)\n",
    "    \n",
    "    # Create the figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot bars for each metric\n",
    "    for i in range(num_metrics):\n",
    "        values = [results[model][i] for model in models]  # Get metric[i] for all models\n",
    "        plt.barh(models, values, height=0.3, label=metrics_names[i], left=[0]*len(models))\n",
    "        \n",
    "        # Add value labels to each bar\n",
    "        for j, value in enumerate(values):\n",
    "            plt.text(value + 0.3, j, f\"{value:.1f}\", va='center', ha='left', fontsize=10)\n",
    "\n",
    "    # Final plot details\n",
    "    plt.title(f\"{dataset_name} Performance Comparison\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.xlim(0, 100 if dataset_name == \"ESC-50\" else 10)  # Adjust scale for the dataset\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for ESC-50\n",
    "plot_horizontal_results(results, \"ESC-50\", metrics_names, \"Accuracy (%)\")\n",
    "\n",
    "# Plot for AudioSet\n",
    "plot_horizontal_results(results, \"AudioSet\", metrics_names, \"mAP (%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Updated function to plot horizontal bar results for multiple metrics\n",
    "def plot_horizontal_results(results, dataset_name, metrics_names, xlabel):\n",
    "    # Extract models (keys) and their corresponding metrics\n",
    "    models = list(results.keys())  # e.g., [\"TED-LIUM\", \"AudioSet\"]\n",
    "    num_metrics = len(metrics_names)\n",
    "    \n",
    "    # Create the figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot bars for each metric\n",
    "    for i in range(num_metrics):\n",
    "        values = [results[model][i] for model in models]  # Get metric[i] for all models\n",
    "        plt.barh(models, values, height=0.3, label=metrics_names[i], left=[0]*len(models))\n",
    "        \n",
    "        # Add value labels to each bar\n",
    "        for j, value in enumerate(values):\n",
    "            plt.text(value + 0.3, j, f\"{value:.1f}\", va='center', ha='left', fontsize=10)\n",
    "\n",
    "    # Final plot details\n",
    "    plt.title(f\"{dataset_name} Performance Comparison\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.xlim(0, 100 if dataset_name == \"ESC-50\" else 10)  # Adjust scale for the dataset\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define results and metric names\n",
    "results = {\n",
    "    \"TED-LIUM\": [78.0, 90.0, 95.0],  # Accuracy, Top-5 Accuracy, Top-10 Accuracy\n",
    "    \"AudioSet\": [72.0, 85.0, 92.0]\n",
    "}\n",
    "metrics_names = [\"Accuracy\", \"Top-5 Accuracy\", \"Top-10 Accuracy\"]\n",
    "\n",
    "# Plot for ESC-50\n",
    "plot_horizontal_results(results, \"ESC-50\", metrics_names, \"Accuracy (%)\")\n",
    "\n",
    "# Plot for AudioSet\n",
    "plot_horizontal_results(results, \"AudioSet\", metrics_names, \"mAP (%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
