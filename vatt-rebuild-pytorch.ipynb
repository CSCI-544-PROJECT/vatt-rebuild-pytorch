{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install necessary libraries\n",
    "!pip -q install torchaudio transformers PySoundFile tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The modified VATT architecture for audio-text alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install and Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Audio Tokenization\n",
    "class AudioTokenizer(nn.Module):\n",
    "    def __init__(self, patch_size=128, embed_dim=512):\n",
    "        super(AudioTokenizer, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Linear(patch_size, embed_dim)\n",
    "    \n",
    "    def forward(self, audio_signal):\n",
    "        # Assuming audio_signal is [batch, time_samples]\n",
    "        batch_size, time_len = audio_signal.shape\n",
    "        num_patches = time_len // self.patch_size\n",
    "        audio_signal = audio_signal[:, :num_patches * self.patch_size]\n",
    "        audio_patches = audio_signal.reshape(batch_size, num_patches, self.patch_size)\n",
    "        # Project each patch to embedding dimension\n",
    "        audio_embeddings = self.projection(audio_patches)  # [batch, num_patches, embed_dim]\n",
    "        return audio_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(nn.Module):\n",
    "    def __init__(self, embed_dim=512, max_length=512):\n",
    "        super(TextTokenizer, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, embed_dim)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text is a list of strings\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens[\"input_ids\"].to(device)  # [batch, max_length]\n",
    "        text_embeddings = self.embedding(input_ids)  # [batch, max_length, embed_dim]\n",
    "        return text_embeddings  # Ensures 3D output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape [1, max_len, embed_dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adjust the positional encoding to match the actual input shape\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        pe = self.pe[:, :seq_len, :].expand(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return x + pe.to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Transformer Encoder Components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim * expansion_factor)\n",
    "        self.fc2 = nn.Linear(embed_dim * expansion_factor, embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.activation(self.fc1(x)))\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = FeedForward(embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expect x as [batch, seq_len, embed_dim] -> Transpose for MultiheadAttention\n",
    "        x = x.transpose(0, 1)  # Now [seq_len, batch, embed_dim]\n",
    "        \n",
    "        attn_output = self.mha(x)\n",
    "        x = x + attn_output  # Residual\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + ffn_output  # Residual\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x.transpose(0, 1)  # Transpose back to [batch, seq_len, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Projection Head\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, proj_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.projection = nn.Linear(embed_dim, proj_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract [CLS] token (or average pool)\n",
    "        x = x[:, 0]  # Assuming first token is [CLS]\n",
    "        return self.projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Contrastive Learning (NCE and MIL-NCE)\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, features_a, features_b):\n",
    "        # Normalize features and compute similarity matrix\n",
    "        features_a = nn.functional.normalize(features_a, dim=1)\n",
    "        features_b = nn.functional.normalize(features_b, dim=1)\n",
    "        logits = torch.matmul(features_a, features_b.T) / self.temperature\n",
    "        labels = torch.arange(len(features_a)).to(device)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: DropToken Implementation\n",
    "class DropToken(nn.Module):\n",
    "    def __init__(self, drop_rate=0.5):\n",
    "        super(DropToken, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_rate\n",
    "        mask = torch.rand(x.shape[:2], device=x.device) < keep_prob\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assuming we have already downloaded TED-LIUM dataset and extracted audio + text files\n",
    "# From: https://www.openslr.org/51/ [50.6 GB]\n",
    "PATH_TO_AUDIO_FILES = \"./audio_files\"\n",
    "PATH_TO_TRANSCRIPT_FILES = \"./transcript_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Imports\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch, max_audio_length=160000):\n",
    "    audio_tensors = []\n",
    "    text_list = []\n",
    "\n",
    "    for audio, text in batch:\n",
    "        # Adjust audio length to max_audio_length\n",
    "        if audio.size(0) > max_audio_length:\n",
    "            audio = audio[:max_audio_length]\n",
    "        else:\n",
    "            audio = F.pad(audio, (0, max_audio_length - audio.size(0)))\n",
    "        \n",
    "        audio_tensors.append(audio)\n",
    "        text_list.append(text)\n",
    "\n",
    "    # Stack the audio tensors into a batch\n",
    "    audio_batch = torch.stack(audio_tensors)\n",
    "\n",
    "    # text_list is a list of strings\n",
    "    return audio_batch, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: TED-LIUM Dataset Setup\n",
    "class TEDLIUMDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transcript_dir, tokenizer, max_text_length=512, sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Initialize dataset with paths to audio and transcript directories and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            audio_dir (str): Path to directory containing audio files.\n",
    "            transcript_dir (str): Path to directory containing transcript files.\n",
    "            tokenizer (transformers.AutoTokenizer): Tokenizer for text data.\n",
    "            max_text_length (int): Maximum length for text tokenization.\n",
    "            sample_rate (int): Desired sample rate for audio.\n",
    "        \"\"\"\n",
    "        self.audio_files = sorted([\n",
    "            os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith(\".wav\")\n",
    "        ])\n",
    "        self.transcript_files = sorted([\n",
    "            os.path.join(transcript_dir, f) for f in os.listdir(transcript_dir) if f.endswith(\".stm\")\n",
    "        ])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file\n",
    "        audio_path = self.audio_files[idx]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample to desired sample rate if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # Load transcript with specified encoding\n",
    "        try:\n",
    "            with open(self.transcript_files[idx], 'r', encoding='utf-8') as f:\n",
    "                transcript = f.read().strip()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"UnicodeDecodeError for file: {self.transcript_files[idx]}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            # Optionally, handle the error by skipping the file or using a fallback\n",
    "            transcript = \"\"  # Assign an empty string or any default value\n",
    "        \n",
    "        # Return waveform and raw text\n",
    "        return waveform.squeeze(0), transcript\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = TEDLIUMDataset(audio_dir=PATH_TO_AUDIO_FILES, transcript_dir=PATH_TO_TRANSCRIPT_FILES, tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, drop_last=True, collate_fn=lambda x: collate_fn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Training Components\n",
    "# Assuming VATT model, projection heads, contrastive loss, and DropToken components are defined as above\n",
    "\n",
    "# Initialize model components\n",
    "audio_tokenizer = AudioTokenizer(patch_size=128, embed_dim=512).to(device)\n",
    "text_tokenizer = TextTokenizer(embed_dim=512, max_length=512).to(device)\n",
    "positional_encoding_audio = PositionalEncoding(embed_dim=512).to(device)\n",
    "positional_encoding_text = PositionalEncoding(embed_dim=512).to(device)\n",
    "transformer_audio = TransformerEncoderLayer(embed_dim=512, num_heads=8).to(device)\n",
    "transformer_text = TransformerEncoderLayer(embed_dim=512, num_heads=8).to(device)\n",
    "projection_head = ProjectionHead(embed_dim=512, proj_dim=256).to(device)\n",
    "contrastive_loss = ContrastiveLoss().to(device)\n",
    "droptoken = DropToken(drop_rate=0.5).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': audio_tokenizer.parameters()},\n",
    "    {'params': text_tokenizer.parameters()},\n",
    "    {'params': positional_encoding_audio.parameters()},\n",
    "    {'params': positional_encoding_text.parameters()},\n",
    "    {'params': transformer_audio.parameters()},\n",
    "    {'params': transformer_text.parameters()},\n",
    "    {'params': projection_head.parameters()}\n",
    "], lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 293/293 [01:45<00:00,  2.77batch/s, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 2.1427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 293/293 [01:16<00:00,  3.84batch/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Average Loss: 2.0804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 293/293 [01:15<00:00,  3.89batch/s, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Average Loss: 2.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 293/293 [01:15<00:00,  3.89batch/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Average Loss: 2.0799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 293/293 [01:15<00:00,  3.87batch/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Average Loss: 2.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 293/293 [01:14<00:00,  3.91batch/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Average Loss: 2.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 293/293 [01:15<00:00,  3.89batch/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Average Loss: 2.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 293/293 [01:15<00:00,  3.87batch/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Average Loss: 2.0812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 293/293 [01:15<00:00,  3.89batch/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Average Loss: 2.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 293/293 [01:15<00:00,  3.90batch/s, loss=2.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Average Loss: 2.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Step 5: Training Loop with Progress Bars for Steps within Each Epoch\n",
    "def train(model_components, dataloader, optimizer, num_epochs=10):\n",
    "    (\n",
    "        audio_tokenizer,\n",
    "        text_tokenizer,\n",
    "        positional_encoding_audio,\n",
    "        positional_encoding_text,\n",
    "        transformer_audio,\n",
    "        transformer_text,\n",
    "        projection_head,\n",
    "        contrastive_loss,\n",
    "        droptoken\n",
    "    ) = model_components\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # Initialize progress bar for batches within the current epoch\n",
    "        batch_progress = tqdm(\n",
    "            dataloader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "            unit=\"batch\",\n",
    "            leave=True  # Keeps the progress bar after the epoch completes\n",
    "        )\n",
    "        \n",
    "        for i, (audio, text) in enumerate(batch_progress):\n",
    "            # Move audio to device; text is a list of strings\n",
    "            audio = audio.to(device)\n",
    "            # text remains on CPU\n",
    "    \n",
    "            # Audio processing\n",
    "            audio_embeddings = audio_tokenizer(audio)  # [batch, num_patches, embed_dim]\n",
    "            audio_embeddings = positional_encoding_audio(audio_embeddings)\n",
    "            audio_embeddings = transformer_audio(audio_embeddings)  # [batch, num_patches, embed_dim]\n",
    "    \n",
    "            # Text processing\n",
    "            text_embeddings = text_tokenizer(text)  # [batch, max_length, embed_dim]\n",
    "            text_embeddings = positional_encoding_text(text_embeddings)  # [batch, max_length, embed_dim]\n",
    "            text_embeddings = transformer_text(text_embeddings)  # [batch, max_length, embed_dim]\n",
    "    \n",
    "            # Apply DropToken during training\n",
    "            audio_embeddings = droptoken(audio_embeddings)\n",
    "            text_embeddings = droptoken(text_embeddings)\n",
    "    \n",
    "            # Project to common space\n",
    "            audio_proj = projection_head(audio_embeddings)  # [batch, proj_dim]\n",
    "            text_proj = projection_head(text_embeddings)    # [batch, proj_dim]\n",
    "    \n",
    "            # Contrastive Loss\n",
    "            loss = contrastive_loss(audio_proj, text_proj)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Update the progress bar's postfix with the current loss every 10 steps\n",
    "            if (i + 1) % 10 == 0:\n",
    "                batch_progress.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Define model components as a tuple\n",
    "model_components = (\n",
    "    audio_tokenizer,\n",
    "    text_tokenizer,\n",
    "    positional_encoding_audio,\n",
    "    positional_encoding_text,\n",
    "    transformer_audio,\n",
    "    transformer_text,\n",
    "    projection_head,\n",
    "    contrastive_loss,\n",
    "    droptoken\n",
    ")\n",
    "\n",
    "# Start training\n",
    "num_epochs = 10\n",
    "train(model_components, dataloader, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained-10-1730201113.9804668.pth\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def save_model(model_components, optimizer, epoch):\n",
    "    path=f\"trained-{epoch}-{time.time()}.pth\"\n",
    "\n",
    "    # Unpack the components\n",
    "    audio_tokenizer, text_tokenizer, positional_encoding_audio, positional_encoding_text, transformer_audio, transformer_text, projection_head, contrastive_loss, droptoken = model_components\n",
    "\n",
    "    # Save state dictionaries\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'audio_tokenizer_state_dict': audio_tokenizer.state_dict(),\n",
    "        'text_tokenizer_state_dict': text_tokenizer.state_dict(),\n",
    "        'positional_encoding_audio_state_dict': positional_encoding_audio.state_dict(),\n",
    "        'positional_encoding_text_state_dict': positional_encoding_text.state_dict(),\n",
    "        'transformer_audio_state_dict': transformer_audio.state_dict(),\n",
    "        'transformer_text_state_dict': transformer_text.state_dict(),\n",
    "        'projection_head_state_dict': projection_head.state_dict(),\n",
    "        'contrastive_loss_state_dict': contrastive_loss.state_dict(),\n",
    "        'droptoken_state_dict': droptoken.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# Call the save function after training\n",
    "save_model(model_components, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
