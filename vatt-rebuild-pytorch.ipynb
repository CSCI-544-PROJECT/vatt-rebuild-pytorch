{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\jayas\\AppData\\Roaming\\Python\\Python312\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Install necessary libraries\n",
    "!pip -q install torchaudio transformers PySoundFile tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The modified VATT architecture for audio-text alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install and Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Audio Tokenization\n",
    "class AudioTokenizer(nn.Module):\n",
    "    def __init__(self, patch_size=128, embed_dim=2048):\n",
    "        super(AudioTokenizer, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Linear(patch_size, embed_dim)\n",
    "    \n",
    "    def forward(self, audio_signal):\n",
    "        # Assuming audio_signal is [batch, time_samples]\n",
    "        batch_size, time_len = audio_signal.shape\n",
    "        num_patches = time_len // self.patch_size\n",
    "        audio_signal = audio_signal[:, :num_patches * self.patch_size]\n",
    "        audio_patches = audio_signal.reshape(batch_size, num_patches, self.patch_size)\n",
    "        # Project each patch to embedding dimension\n",
    "        audio_embeddings = self.projection(audio_patches)  # [batch, num_patches, embed_dim]\n",
    "        return audio_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(nn.Module):\n",
    "    def __init__(self, embed_dim=768, max_length=512):\n",
    "        super(TextTokenizer, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, embed_dim)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text is a list of strings\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens[\"input_ids\"].to(device)  # [batch, max_length]\n",
    "        text_embeddings = self.embedding(input_ids)  # [batch, max_length, embed_dim]\n",
    "        return text_embeddings  # Ensures 3D output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # Shape [1, max_len, embed_dim]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adjust the positional encoding to match the actual input shape\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        pe = self.pe[:, :seq_len, :].expand(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return x + pe.to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Transformer Encoder Components\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim * expansion_factor)\n",
    "        self.fc2 = nn.Linear(embed_dim * expansion_factor, embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.activation(self.fc1(x)))\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.linear1 = nn.Linear(embed_dim, embed_dim * 4)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear2 = nn.Linear(embed_dim * 4, embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFactory(nn.Module):\n",
    "    \"\"\"\n",
    "    Factory class to create TransformerEncoder instances with customizable parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, feedforward_dim_multiplier=4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): Dimension of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            num_layers (int): Number of Transformer layers.\n",
    "            feedforward_dim_multiplier (int): Multiplier for feedforward layer dimensions.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(TransformerFactory, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim, \n",
    "                nhead=num_heads, \n",
    "                dim_feedforward=embed_dim * feedforward_dim_multiplier, \n",
    "                dropout=dropout,\n",
    "                batch_first=True  # Enable batch-first optimization\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Projection Head\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, proj_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.hidden_dim = embed_dim * 2\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embed_dim, self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_dim, proj_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract [CLS] token (or average pool)\n",
    "        x = x[:, 0]  # Assuming first token is [CLS]\n",
    "        return self.projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Contrastive Learning (NCE and MIL-NCE)\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, features_a, features_b):\n",
    "        # Normalize features and compute similarity matrix\n",
    "        features_a = nn.functional.normalize(features_a, dim=1)\n",
    "        features_b = nn.functional.normalize(features_b, dim=1)\n",
    "        logits = torch.matmul(features_a, features_b.T) / self.temperature\n",
    "        labels = torch.arange(len(features_a)).to(device)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: DropToken Implementation\n",
    "class DropToken(nn.Module):\n",
    "    def __init__(self, drop_rate=0.5):\n",
    "        super(DropToken, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_rate\n",
    "        mask = torch.rand(x.shape[:2], device=x.device) < keep_prob\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Assuming we have already downloaded TED-LIUM dataset and extracted audio + text files\n",
    "# From: https://www.openslr.org/51/ [50.6 GB]\n",
    "PATH_TO_AUDIO_FILES = \"./audio_files\"\n",
    "PATH_TO_TRANSCRIPT_FILES = \"./transcript_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Imports\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, AutoTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch, max_audio_length=160000):\n",
    "    audio_tensors = []\n",
    "    text_list = []\n",
    "\n",
    "    for audio, text in batch:\n",
    "        # Adjust audio length to max_audio_length\n",
    "        if audio.size(0) > max_audio_length:\n",
    "            audio = audio[:max_audio_length]\n",
    "        else:\n",
    "            audio = F.pad(audio, (0, max_audio_length - audio.size(0)))\n",
    "        \n",
    "        audio_tensors.append(audio)\n",
    "        text_list.append(text)\n",
    "\n",
    "    # Stack the audio tensors into a batch\n",
    "    audio_batch = torch.stack(audio_tensors)\n",
    "\n",
    "    # text_list is a list of strings\n",
    "    return audio_batch, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: TED-LIUM Dataset Setup\n",
    "class TEDLIUMDataset(Dataset):\n",
    "    def __init__(self, audio_dir, transcript_dir, tokenizer, max_text_length=512, sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Initialize dataset with paths to audio and transcript directories and tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            audio_dir (str): Path to directory containing audio files.\n",
    "            transcript_dir (str): Path to directory containing transcript files.\n",
    "            tokenizer (transformers.AutoTokenizer): Tokenizer for text data.\n",
    "            max_text_length (int): Maximum length for text tokenization.\n",
    "            sample_rate (int): Desired sample rate for audio.\n",
    "        \"\"\"\n",
    "        self.audio_files = sorted([\n",
    "            os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith(\".wav\")\n",
    "        ])\n",
    "        self.transcript_files = sorted([\n",
    "            os.path.join(transcript_dir, f) for f in os.listdir(transcript_dir) if f.endswith(\".stm\")\n",
    "        ])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file\n",
    "        audio_path = self.audio_files[idx]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample to desired sample rate if necessary\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # Load transcript with specified encoding\n",
    "        try:\n",
    "            with open(self.transcript_files[idx], 'r', encoding='utf-8') as f:\n",
    "                transcript = f.read().strip()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"UnicodeDecodeError for file: {self.transcript_files[idx]}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "            # Optionally, handle the error by skipping the file or using a fallback\n",
    "            transcript = \"\"  # Assign an empty string or any default value\n",
    "        \n",
    "        # Return waveform and raw text\n",
    "        return waveform.squeeze(0), transcript\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = TEDLIUMDataset(audio_dir=PATH_TO_AUDIO_FILES, transcript_dir=PATH_TO_TRANSCRIPT_FILES, tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True, collate_fn=lambda x: collate_fn(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define Training Components\n",
    "\n",
    "# Initialize model components\n",
    "from transformers import Wav2Vec2Model, BertModel\n",
    "\n",
    "\"\"\"\n",
    "audio_tokenizer = AudioTokenizer(patch_size=128, embed_dim=2048).to(device)\n",
    "text_tokenizer = TextTokenizer(embed_dim=768, max_length=512).to(device)\n",
    "\n",
    "positional_encoding_audio = PositionalEncoding(embed_dim=2048).to(device)\n",
    "positional_encoding_text = PositionalEncoding(embed_dim=768).to(device)\n",
    "\n",
    "transformer_audio = TransformerFactory(embed_dim=2048, num_heads=8, num_layers=2, feedforward_dim_multiplier=2).to(device)\n",
    "transformer_text = TransformerFactory(embed_dim=768, num_heads=8, num_layers=12, feedforward_dim_multiplier=6).to(device)\n",
    "\"\"\"\n",
    "\n",
    "audio_encoder = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-large-960h').to(device)\n",
    "audio_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "text_encoder = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "text_encoder.gradient_checkpointing_enable()\n",
    "\n",
    "projection_head_audio = ProjectionHead(embed_dim=1024, proj_dim=256).to(device)\n",
    "projection_head_text = ProjectionHead(embed_dim=768, proj_dim=256).to(device)\n",
    "\n",
    "# Example for BERT model\n",
    "\n",
    "# Example for Wav2Vec2 model\n",
    "\n",
    "contrastive_loss = ContrastiveLoss().to(device)\n",
    "droptoken = DropToken(drop_rate=0.5).to(device)\n",
    "\n",
    "# Define a lower learning rate for pretrained encoders\n",
    "pretrained_lr = 1e-5  # Lower learning rate for BERT and Wav2Vec2\n",
    "training_lr = 1e-4  # Default learning rate for the rest of the model\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW([\n",
    "    # {'params': audio_tokenizer.parameters()},\n",
    "    # {'params': text_tokenizer.parameters()},\n",
    "    # {'params': positional_encoding_audio.parameters()},\n",
    "    # {'params': positional_encoding_text.parameters()},\n",
    "    # {'params': transformer_audio.parameters()},\n",
    "    # {'params': transformer_text.parameters()},\n",
    "    {'params': audio_encoder.parameters(), 'lr': pretrained_lr},\n",
    "    {'params': text_encoder.parameters(), 'lr': pretrained_lr},\n",
    "    {'params': projection_head_audio.parameters(), 'lr': training_lr},\n",
    "    {'params': projection_head_text.parameters(), 'lr': training_lr},\n",
    "], lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  12%|█▏        | 68/587 [1:35:35<8:26:49, 58.59s/batch, loss=1.39]  "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# Step 5: Training Loop with Progress Bars for Steps within Each Epoch\n",
    "def train(model_components, dataloader, optimizer, num_epochs=10):\n",
    "    (\n",
    "        audio_encoder,\n",
    "        text_encoder,\n",
    "        projection_head_audio,\n",
    "        projection_head_text,\n",
    "        contrastive_loss,\n",
    "        droptoken\n",
    "    ) = model_components\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # Initialize progress bar for batches within the current epoch\n",
    "        batch_progress = tqdm(\n",
    "            dataloader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "            unit=\"batch\",\n",
    "            leave=True  # Keeps the progress bar after the epoch completes\n",
    "        )\n",
    "        \n",
    "        accumulation_steps = 4\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        for i, (audio, text) in enumerate(batch_progress):\n",
    "            # Move audio to device; text remains on CPU\n",
    "            audio = audio.to(device)\n",
    "\n",
    "            # Audio processing with Wav2Vec2\n",
    "            audio_output = audio_encoder(audio)  # [batch, seq_len, 768]\n",
    "            audio_embeddings = audio_output.last_hidden_state  # Extract the hidden states\n",
    "            audio_embeddings = droptoken(audio_embeddings)     # Apply DropToken\n",
    "            audio_proj = projection_head_audio(audio_embeddings)  # Project to common space\n",
    "\n",
    "            # Text processing with BERT\n",
    "            tokenized_text = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            text_output = text_encoder(**tokenized_text)  # [batch, seq_len, 768]\n",
    "            text_embeddings = text_output.last_hidden_state  # Extract the hidden states\n",
    "            text_embeddings = droptoken(text_embeddings)     # Apply DropToken\n",
    "            text_proj = projection_head_text(text_embeddings)  # Project to common space\n",
    "\n",
    "            # Contrastive Loss\n",
    "            loss = contrastive_loss(audio_proj, text_proj)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Backpropagation\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)  # Apply the gradients\n",
    "                scaler.update()          # Update the scaler\n",
    "                optimizer.zero_grad()    # Reset gradients\n",
    "            # No need to call scaler.update() here since it's done after step()\n",
    "\n",
    "            # Update the progress bar's postfix with the current loss every 10 steps\n",
    "            if (i + 1) % 10 == 0:\n",
    "                batch_progress.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Define model components as a tuple\n",
    "model_components = (\n",
    "    audio_encoder,\n",
    "    text_encoder,\n",
    "    projection_head_audio,\n",
    "    projection_head_text,\n",
    "    contrastive_loss,\n",
    "    droptoken\n",
    ")\n",
    "\n",
    "# Start training\n",
    "num_epochs = 10\n",
    "train(model_components, dataloader, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained-10-1732919613.250842.pth\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def save_model(model_components, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Save the state dictionaries of all model components and the optimizer.\n",
    "\n",
    "    Args:\n",
    "        model_components (tuple): Tuple containing all model components.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used during training.\n",
    "        epoch (int): The current epoch number.\n",
    "    \"\"\"\n",
    "    # Generate a unique filename for the saved model\n",
    "    path = f\"trained-{epoch}-{time.time()}.pth\"\n",
    "\n",
    "    # Unpack the components\n",
    "    (\n",
    "        audio_tokenizer,\n",
    "        text_tokenizer,\n",
    "        positional_encoding_audio,\n",
    "        positional_encoding_text,\n",
    "        transformer_audio,\n",
    "        transformer_text,\n",
    "        projection_head_audio,\n",
    "        projection_head_text,\n",
    "        contrastive_loss,\n",
    "        droptoken\n",
    "    ) = model_components\n",
    "\n",
    "    # Save state dictionaries\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'audio_tokenizer_state_dict': audio_tokenizer.state_dict(),\n",
    "        'text_tokenizer_state_dict': text_tokenizer.state_dict(),\n",
    "        'positional_encoding_audio_state_dict': positional_encoding_audio.state_dict(),\n",
    "        'positional_encoding_text_state_dict': positional_encoding_text.state_dict(),\n",
    "        'transformer_audio_state_dict': transformer_audio.state_dict(),\n",
    "        'transformer_text_state_dict': transformer_text.state_dict(),\n",
    "        'projection_head_audio_state_dict': projection_head_audio.state_dict(),\n",
    "        'projection_head_text_state_dict': projection_head_text.state_dict(),\n",
    "        'contrastive_loss_state_dict': contrastive_loss.state_dict(),\n",
    "        'droptoken_state_dict': droptoken.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "save_model(model_components, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component                          Parameters\n",
      "--------------------------------------------------\n",
      "Audio Tokenizer                       264,192\n",
      "Text Tokenizer                     23,440,896\n",
      "Positional Encoding (Audio)                 0\n",
      "Positional Encoding (Text)                  0\n",
      "Transformer (Audio)                67,153,920\n",
      "Transformer (Text)                113,384,448\n",
      "Projection Head (Audio)             9,441,536\n",
      "Projection Head (Text)              1,574,656\n",
      "Contrastive Loss                            0\n",
      "DropToken                                   0\n",
      "--------------------------------------------------\n",
      "Total Trainable Parameters        215,259,648\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model_components):\n",
    "    \"\"\"\n",
    "    Counts and prints the total number of trainable parameters for each model component.\n",
    "\n",
    "    Args:\n",
    "        model_components (tuple): A tuple of PyTorch model components.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    print(f\"{'Component':<30}{'Parameters':>15}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Iterate through each component in the tuple\n",
    "    component_names = [\n",
    "        \"Audio Tokenizer\",\n",
    "        \"Text Tokenizer\",\n",
    "        \"Positional Encoding (Audio)\",\n",
    "        \"Positional Encoding (Text)\",\n",
    "        \"Transformer (Audio)\",\n",
    "        \"Transformer (Text)\",\n",
    "        \"Projection Head (Audio)\",\n",
    "        \"Projection Head (Text)\",\n",
    "        \"Contrastive Loss\",\n",
    "        \"DropToken\"\n",
    "    ]\n",
    "\n",
    "    for name, component in zip(component_names, model_components):\n",
    "        if hasattr(component, \"parameters\"):\n",
    "            component_params = sum(p.numel() for p in component.parameters() if p.requires_grad)\n",
    "            total_params += component_params\n",
    "            print(f\"{name:<30}{component_params:>15,}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Total Trainable Parameters':<30}{total_params:>15,}\")\n",
    "    return total_params\n",
    "\n",
    "total_params = count_parameters(model_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLAP parameter sizes**\n",
    "\n",
    "Total parameters: 196,304,144  \n",
    "Trainable parameters: 195,220,688  \n",
    "Text encoder weights have been frozen.  \n",
    "Audio Encoder - Total parameters: 84,984,847  \n",
    "Audio Encoder - Trainable parameters: 83,901,391  \n",
    "Text Encoder - Total parameters: 111,319,296  \n",
    "Text Encoder - Trainable parameters: 1,837,056  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
